{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rohan581/Dashtoon_Style_Transfer/blob/main/Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "import torch\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import re\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import torch.onnx\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "h3ts7j3SkoU2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install  g"
      ],
      "metadata": {
        "id": "2_yAU11u59il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vgg16(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Vgg16, self).__init__()\n",
        "        features = models.vgg16(pretrained=True).features\n",
        "        self.to_relu_1_2 = nn.Sequential()\n",
        "        self.to_relu_2_2 = nn.Sequential()\n",
        "        self.to_relu_3_3 = nn.Sequential()\n",
        "        self.to_relu_4_3 = nn.Sequential()\n",
        "\n",
        "        for x in range(4):\n",
        "            self.to_relu_1_2.add_module(str(x), features[x])\n",
        "        for x in range(4, 9):\n",
        "            self.to_relu_2_2.add_module(str(x), features[x])\n",
        "        for x in range(9, 16):\n",
        "            self.to_relu_3_3.add_module(str(x), features[x])\n",
        "        for x in range(16, 23):\n",
        "            self.to_relu_4_3.add_module(str(x), features[x])\n",
        "\n",
        "        # don't need the gradients, just want the features\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.to_relu_1_2(x)\n",
        "        h_relu_1_2 = h\n",
        "        h = self.to_relu_2_2(h)\n",
        "        h_relu_2_2 = h\n",
        "        h = self.to_relu_3_3(h)\n",
        "        h_relu_3_3 = h\n",
        "        h = self.to_relu_4_3(h)\n",
        "        h_relu_4_3 = h\n",
        "        out = (h_relu_1_2, h_relu_2_2, h_relu_3_3, h_relu_4_3)\n",
        "        return out"
      ],
      "metadata": {
        "id": "zULZyQmZlBGR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image"
      ],
      "metadata": {
        "id": "KSsIbdgno-H4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(filename):\n",
        "    img = Image.open(filename)\n",
        "    return img\n",
        "\n",
        "# assumes data comes in batch form (ch, h, w)\n",
        "def save_image(filename, data):\n",
        "    std = np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))\n",
        "    mean = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))\n",
        "    img = data.clone().numpy()\n",
        "    img = ((img * std + mean).transpose(1, 2, 0)*255.0).clip(0, 255).astype(\"uint8\")\n",
        "    img = Image.fromarray(img)\n",
        "    img.save(filename)\n",
        "\n",
        "# Calculate Gram matrix (G = FF^T)\n",
        "def gram(x):\n",
        "    (bs, ch, h, w) = x.size()\n",
        "    f = x.view(bs, ch, w*h)\n",
        "    f_T = f.transpose(1, 2)\n",
        "    G = f.bmm(f_T) / (ch * h * w)\n",
        "    return G\n",
        "\n",
        "# using ImageNet values\n",
        "def normalize_tensor_transform():\n",
        "    return transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])"
      ],
      "metadata": {
        "id": "9WXh7gjPpLg7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        padding = kernel_size // 2\n",
        "        self.reflection_pad = nn.ReflectionPad2d(padding)\n",
        "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride) #, padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.reflection_pad(x)\n",
        "        out = self.conv2d(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "ObfYlp7WvSvv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
        "        self.in1 = nn.InstanceNorm2d(channels, affine=True)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
        "        self.in2 = nn.InstanceNorm2d(channels, affine=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.relu(self.in1(self.conv1(x)))\n",
        "        out = self.in2(self.conv2(out))\n",
        "        out = out + residual\n",
        "        out = self.relu(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "lc-xrlnOvitV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpsampleConvLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
        "        super(UpsampleConvLayer, self).__init__()\n",
        "        self.upsample = upsample\n",
        "        if upsample:\n",
        "            self.upsample = nn.Upsample(scale_factor=upsample, mode='nearest')\n",
        "        reflection_padding = kernel_size // 2\n",
        "        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n",
        "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.upsample:\n",
        "            x = self.upsample(x)\n",
        "        out = self.reflection_pad(x)\n",
        "        out = self.conv2d(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "Y2USbs3rv23r"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageTransformNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageTransformNet, self).__init__()\n",
        "\n",
        "        # nonlineraity\n",
        "        self.relu = nn.ReLU()\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        # encoding layers\n",
        "        self.conv1 = ConvLayer(3, 32, kernel_size=9, stride=1)\n",
        "        self.in1_e = nn.InstanceNorm2d(32, affine=True)\n",
        "\n",
        "        self.conv2 = ConvLayer(32, 64, kernel_size=3, stride=2)\n",
        "        self.in2_e = nn.InstanceNorm2d(64, affine=True)\n",
        "\n",
        "        self.conv3 = ConvLayer(64, 128, kernel_size=3, stride=2)\n",
        "        self.in3_e = nn.InstanceNorm2d(128, affine=True)\n",
        "\n",
        "        # residual layers\n",
        "        self.res1 = ResidualBlock(128)\n",
        "        self.res2 = ResidualBlock(128)\n",
        "        self.res3 = ResidualBlock(128)\n",
        "        self.res4 = ResidualBlock(128)\n",
        "        self.res5 = ResidualBlock(128)\n",
        "\n",
        "        # decoding layers\n",
        "        self.deconv3 = UpsampleConvLayer(128, 64, kernel_size=3, stride=1, upsample=2 )\n",
        "        self.in3_d = nn.InstanceNorm2d(64, affine=True)\n",
        "\n",
        "        self.deconv2 = UpsampleConvLayer(64, 32, kernel_size=3, stride=1, upsample=2 )\n",
        "        self.in2_d = nn.InstanceNorm2d(32, affine=True)\n",
        "\n",
        "        self.deconv1 = UpsampleConvLayer(32, 3, kernel_size=9, stride=1)\n",
        "        self.in1_d = nn.InstanceNorm2d(3, affine=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encode\n",
        "        y = self.relu(self.in1_e(self.conv1(x)))\n",
        "        y = self.relu(self.in2_e(self.conv2(y)))\n",
        "        y = self.relu(self.in3_e(self.conv3(y)))\n",
        "\n",
        "        # residual layers\n",
        "        y = self.res1(y)\n",
        "        y = self.res2(y)\n",
        "        y = self.res3(y)\n",
        "        y = self.res4(y)\n",
        "        y = self.res5(y)\n",
        "\n",
        "        # decode\n",
        "        y = self.relu(self.in3_d(self.deconv3(y)))\n",
        "        y = self.relu(self.in2_d(self.deconv2(y)))\n",
        "        y = self.tanh(self.in1_d(self.deconv1(y)))\n",
        "        #y = self.deconv1(y)\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "CIxlehyntSnR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 256\n",
        "BATCH_SIZE = 4\n",
        "LEARNING_RATE = 1e-3\n",
        "EPOCHS = 2\n",
        "STYLE_WEIGHT = 1e5\n",
        "CONTENT_WEIGHT = 1e0\n",
        "TV_WEIGHT = 1e-7"
      ],
      "metadata": {
        "id": "K7sKDMYdESw3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(args):\n",
        "    if (args['gpu'] != None):\n",
        "      use_cuda = True\n",
        "      dtype = torch.cuda.FloatTensor\n",
        "      torch.cuda.set_device(args['gpu'])\n",
        "      print(\"Current device: %d\" %torch.cuda.current_device())\n",
        "\n",
        "    # visualization of training controlled by flag\n",
        "    visualize = (args['visualize'] != None)\n",
        "    if (visualize):\n",
        "        img_transform_512 = transforms.Compose([\n",
        "            transforms.Scale(512),                  # scale shortest side to image_size\n",
        "            transforms.CenterCrop(512),             # crop center image_size out\n",
        "            transforms.ToTensor(),                  # turn image from [0-255] to [0-1]\n",
        "            normalize_tensor_transform()      # normalize with ImageNet values\n",
        "        ])\n",
        "\n",
        "        testImage_amber = load_image(\"content_imgs/amber.jpg\")\n",
        "        testImage_amber = img_transform_512(testImage_amber)\n",
        "        testImage_amber = Variable(testImage_amber.repeat(1, 1, 1, 1), requires_grad=False).type(dtype)\n",
        "\n",
        "        testImage_dan = load_image(\"content_imgs/dan.jpg\")\n",
        "        testImage_dan = img_transform_512(testImage_dan)\n",
        "        testImage_dan = Variable(testImage_dan.repeat(1, 1, 1, 1), requires_grad=False).type(dtype)\n",
        "\n",
        "        testImage_maine = load_image(\"content_imgs/maine.jpg\")\n",
        "        testImage_maine = img_transform_512(testImage_maine)\n",
        "        testImage_maine = Variable(testImage_maine.repeat(1, 1, 1, 1), requires_grad=False).type(dtype)\n",
        "\n",
        "    # define network\n",
        "    image_transformer = ImageTransformNet().type(dtype)\n",
        "    optimizer = Adam(image_transformer.parameters(), LEARNING_RATE)\n",
        "\n",
        "    loss_mse = torch.nn.MSELoss()\n",
        "\n",
        "    # load vgg network\n",
        "    vgg = Vgg16().type(dtype)\n",
        "\n",
        "    # get training dataset\n",
        "    dataset_transform = transforms.Compose([\n",
        "        transforms.Scale(IMAGE_SIZE),           # scale shortest side to image_size\n",
        "        transforms.CenterCrop(IMAGE_SIZE),      # crop center image_size out\n",
        "        transforms.ToTensor(),                  # turn image from [0-255] to [0-1]\n",
        "        normalize_tensor_transform()      # normalize with ImageNet values\n",
        "    ])\n",
        "    train_dataset = datasets.ImageFolder(args['dataset'], dataset_transform)\n",
        "    train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE)\n",
        "\n",
        "    # style image\n",
        "    style_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),                  # turn image from [0-255] to [0-1]\n",
        "        normalize_tensor_transform()      # normalize with ImageNet values\n",
        "    ])\n",
        "    style = load_image(args['style_image'])\n",
        "    style = style_transform(style)\n",
        "    style = Variable(style.repeat(BATCH_SIZE, 1, 1, 1)).type(dtype)\n",
        "    style_name = os.path.split(args['style_image'])[-1].split('.')[0]\n",
        "\n",
        "    # calculate gram matrices for style feature layer maps we care about\n",
        "    style_features = vgg(style)\n",
        "    style_gram = [gram(fmap) for fmap in style_features]\n",
        "\n",
        "    for e in range(EPOCHS):\n",
        "\n",
        "        # track values for...\n",
        "        img_count = 0\n",
        "        aggregate_style_loss = 0.0\n",
        "        aggregate_content_loss = 0.0\n",
        "        aggregate_tv_loss = 0.0\n",
        "\n",
        "        # train network\n",
        "        image_transformer.train()\n",
        "        for batch_num, (x, label) in enumerate(train_loader):\n",
        "            img_batch_read = len(x)\n",
        "            img_count += img_batch_read\n",
        "\n",
        "            # zero out gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # input batch to transformer network\n",
        "            x = Variable(x).type(dtype)\n",
        "            y_hat = image_transformer(x)\n",
        "\n",
        "            # get vgg features\n",
        "            y_c_features = vgg(x)\n",
        "            y_hat_features = vgg(y_hat)\n",
        "\n",
        "            # calculate style loss\n",
        "            y_hat_gram = [gram(fmap) for fmap in y_hat_features]\n",
        "            style_loss = 0.0\n",
        "            for j in range(4):\n",
        "                style_loss += loss_mse(y_hat_gram[j], style_gram[j][:img_batch_read])\n",
        "            style_loss = STYLE_WEIGHT*style_loss\n",
        "            aggregate_style_loss += style_loss.data[0]\n",
        "\n",
        "            # calculate content loss (h_relu_2_2)\n",
        "            recon = y_c_features[1]\n",
        "            recon_hat = y_hat_features[1]\n",
        "            content_loss = CONTENT_WEIGHT*loss_mse(recon_hat, recon)\n",
        "            aggregate_content_loss += content_loss.data[0]\n",
        "\n",
        "            # calculate total variation regularization (anisotropic version)\n",
        "            # https://www.wikiwand.com/en/Total_variation_denoising\n",
        "            diff_i = torch.sum(torch.abs(y_hat[:, :, :, 1:] - y_hat[:, :, :, :-1]))\n",
        "            diff_j = torch.sum(torch.abs(y_hat[:, :, 1:, :] - y_hat[:, :, :-1, :]))\n",
        "            tv_loss = TV_WEIGHT*(diff_i + diff_j)\n",
        "            aggregate_tv_loss += tv_loss.data[0]\n",
        "\n",
        "            # total loss\n",
        "            total_loss = style_loss + content_loss + tv_loss\n",
        "\n",
        "            # backprop\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print out status message\n",
        "            if ((batch_num + 1) % 100 == 0):\n",
        "                status = \"{}  Epoch {}:  [{}/{}]  Batch:[{}]  agg_style: {:.6f}  agg_content: {:.6f}  agg_tv: {:.6f}  style: {:.6f}  content: {:.6f}  tv: {:.6f} \".format(\n",
        "                                time.ctime(), e + 1, img_count, len(train_dataset), batch_num+1,\n",
        "                                aggregate_style_loss/(batch_num+1.0), aggregate_content_loss/(batch_num+1.0), aggregate_tv_loss/(batch_num+1.0),\n",
        "                                style_loss.data[0], content_loss.data[0], tv_loss.data[0]\n",
        "                            )\n",
        "                print(status)\n",
        "\n",
        "            if ((batch_num + 1) % 1000 == 0) and (visualize):\n",
        "                image_transformer.eval()\n",
        "\n",
        "                if not os.path.exists(\"visualization\"):\n",
        "                    os.makedirs(\"visualization\")\n",
        "                if not os.path.exists(\"visualization/%s\" %style_name):\n",
        "                    os.makedirs(\"visualization/%s\" %style_name)\n",
        "\n",
        "                outputTestImage_amber = image_transformer(testImage_amber).cpu()\n",
        "                amber_path = \"visualization/%s/amber_%d_%05d.jpg\" %(style_name, e+1, batch_num+1)\n",
        "                save_image(amber_path, outputTestImage_amber.data[0])\n",
        "\n",
        "                outputTestImage_dan = image_transformer(testImage_dan).cpu()\n",
        "                dan_path = \"visualization/%s/dan_%d_%05d.jpg\" %(style_name, e+1, batch_num+1)\n",
        "                save_image(dan_path, outputTestImage_dan.data[0])\n",
        "\n",
        "                outputTestImage_maine = image_transformer(testImage_maine).cpu()\n",
        "                maine_path = \"visualization/%s/maine_%d_%05d.jpg\" %(style_name, e+1, batch_num+1)\n",
        "                save_image(maine_path, outputTestImage_maine.data[0])\n",
        "\n",
        "                print(\"images saved\")\n",
        "                image_transformer.train()\n",
        "\n",
        "    # save model\n",
        "    image_transformer.eval()\n",
        "\n",
        "    if use_cuda:\n",
        "        image_transformer.cpu()\n",
        "\n",
        "    if not os.path.exists(\"models\"):\n",
        "        os.makedirs(\"models\")\n",
        "    filename = \"models/\" + str(style_name) + \"_\" + str(time.ctime()).replace(' ', '_') + \".model\"\n",
        "    torch.save(image_transformer.state_dict(), filename)\n",
        "\n",
        "    if use_cuda:\n",
        "        image_transformer.cuda()\n",
        "\n",
        "def style_transfer(args):\n",
        "    # GPU enabling\n",
        "    if (args['gpu'] != None):\n",
        "        use_cuda = True\n",
        "        dtype = torch.cuda.FloatTensor\n",
        "        torch.cuda.set_device(args['gpu'])\n",
        "        print(\"Current device: %d\" %torch.cuda.current_device())\n",
        "\n",
        "    # content image\n",
        "    img_transform_512 = transforms.Compose([\n",
        "            transforms.Scale(512),                  # scale shortest side to image_size\n",
        "            transforms.CenterCrop(512),             # crop center image_size out\n",
        "            transforms.ToTensor(),                  # turn image from [0-255] to [0-1]\n",
        "            normalize_tensor_transform()      # normalize with ImageNet values\n",
        "    ])\n",
        "\n",
        "    content = load_image(args['source'])\n",
        "    content = img_transform_512(content)\n",
        "    content = content.unsqueeze(0)\n",
        "    content = Variable(content).type(dtype)\n",
        "\n",
        "    # load style model\n",
        "    style_model = ImageTransformNet().type(dtype)\n",
        "    style_model.load_state_dict(torch.load(args['model_path']))\n",
        "\n",
        "    # process input image\n",
        "    stylized = style_model(content).cpu()\n",
        "    save_image(args['output'], stylized.data[0])\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description='style transfer in pytorch')\n",
        "    subparsers = parser.add_subparsers(title=\"subcommands\", dest=\"subcommand\")\n",
        "\n",
        "    train_parser = subparsers.add_parser(\"train\", help=\"train a model to do style transfer\")\n",
        "    train_parser.add_argument(\"--style-image\", type=str, required=True, help=\"path to a style image to train with\")\n",
        "    train_parser.add_argument(\"--dataset\", type=str, required=True, help=\"path to a dataset\")\n",
        "    train_parser.add_argument(\"--gpu\", type=int, default=None, help=\"ID of GPU to be used\")\n",
        "    train_parser.add_argument(\"--visualize\", type=int, default=None, help=\"Set to 1 if you want to visualize training\")\n",
        "\n",
        "    style_parser = subparsers.add_parser(\"transfer\", help=\"do style transfer with a trained model\")\n",
        "    style_parser.add_argument(\"--model-path\", type=str, required=True, help=\"path to a pretrained model for a style image\")\n",
        "    style_parser.add_argument(\"--source\", type=str, required=True, help=\"path to source image\")\n",
        "    style_parser.add_argument(\"--output\", type=str, required=True, help=\"file name for stylized output image\")\n",
        "    style_parser.add_argument(\"--gpu\", type=int, default=None, help=\"ID of GPU to be used\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # command\n",
        "    if (args.subcommand == \"train\"):\n",
        "        print(\"Training!\")\n",
        "        train(args)\n",
        "    elif (args.subcommand == \"transfer\"):\n",
        "        print(\"Style transfering!\")\n",
        "        style_transfer(args)\n",
        "    else:\n",
        "        print(\"invalid command\")\n",
        "\n"
      ],
      "metadata": {
        "id": "TChjJv8vDNGD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    \"style-image\": \"\",\n",
        "    \"dataset\": \"\",\n",
        "    \"gpu\": \"\",\n",
        "    \"visualize\": \"\",\n",
        "    \"transfer\": \"\",\n",
        "    \"model-path\": \"\",\n",
        "    \"source\": \"\",\n",
        "    \"output\": \"\",\n",
        "    \"gpu\": \"\",\n",
        "    \"train\":\"\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "ElLNAV_4GVPk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}